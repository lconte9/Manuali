# Introduzione 
S3 ( Simple Storage Service ) è uno spazio di memorizzazione "infinito", in pratica è una memoria dimensionabile in maniera semplice ed automatica. Basata sul concetto di oggetto e ha nome univuoco a livello globale ma è una risorsa regionale (definita alla creazione). La convenzione per il nome prevede :
+ No lettere maiuscole
+ No underscore
+ dai 3 ai 64 caratteri
+ Non un IP
+ Deve iniziare con una lettera minuscola o un numero  

Quando instanziamo un file in un bucket esso viene identificato come un oggetto che possiede una chiave, identificata con il path completo del file a partire dalla cartella successiva a quella del bucket completo. la chiave può essere scomposta come il prefisso + nome del file. Un esempio è : `s3://mio-bucket/cartella1/cartella2/file.txt` in questo caso la chiave è : `cartella1/cartella2/file.txt` dove troviamo il prefisso : `cartella1/cartella2` e il nome del file `file.txt`. 
Questa dicitura lacia pensare che ci troviamo in una directory ma non è cosi, quanto scritto identifica solo elementi e non una gerarchia di file e cartelle.
Un oggetto del bucket è costituito da :
+ **L'object value** rappresenta il contenuto della path, il file più grande che S3 possa caricare è di 5TB ma non è posibile fare upload di file maggiori di 5GB quindi in un caso del genere bisognerebbe suddividere l'elemento ed effettuare un caricamento diviso di tutte le parti. 
+ **Metadati** per ogni oggetto del bucket, formati da una coppia chiave(testuale) valore che possono contenere dati di sistema, relativi ad utenti 
+ **Tags** come i metadati sono formati da una coppia chiave(unicode) valore, non se ne possono avere più di 10 per obj e vengono utilizzati per la sicurezza o per definire il ciclo vita dell'Obj. 
+ **Version ID** viene utilizzato per conoscere la versione del obj.

# Configurazione 
Andiamo nella sezione servizi e digitiamo s3. Una volta aperto il servizio vi verranno mostrati tutti i bucket a vostra disposizione. Anche se S3 è globale i singoli bucket sono regionali. Ricordiamo che il nome del bucket è globale quindi se qualcuno ha già utilizzato il nome che volete utilizzare non potrete più usarlo quindi un consiglio e scrivere come parte del nome il dominio che possedete. tra le opzioni troviamo : 
+ Controllo delle versioni : per salvare le versioni degli oggetti nello stesso bucket
+ Salvataggio di log di accesso
+ tag
+ Salvataggio e visualizzazione dell'accesso tramite API con CloudTrail
+ crittografia
+ Applicare cloudwatch
+ permessi di accesso a reti pubbliche 

Una volta creato possiamo caricare file dalla dashboard cliccando sullo specifico bucket.
le impostazioni di upload per singolo file o più file sono :
+ lista di utenti autorizzati per lettura e scrittura tramite l'IAM e per gli accessi pubblici
+ Tipologia di storage
+ tipologia di crittografia
+ metadati

Inoltre possiamo anche inserire configurazioni specifiche per path.

# Versioning
I file caricati possono essere sottoposti a versioning ma devono essere abilitati a livello di bucket. In pratica, se attiviamo il versioning, quando carichiamo una nuova versione del file il vecchio non verrà eliminato ma verrà salvato con un valore di versione cosi da poter essere recuperato. Tutti i file che non sono stati versionati non hanno una loro stato precedente e se si interrompe il versioning i cambiamenti non verranno salvati. l'eliminazione in s3 con il versioning prevede l'applicazione di un marker che se eliminato ripristinerà il file. L'elenco dei file con versione è visibile quando selezioniamo il versioning scegliendo l'opzione show, vedremo per ogni file l'elenco delle modifiche e in loro id.

# Encription
AWS mette a disposizione 4 metodi per criptare i propri dati e sono :
+ SSE-S3 : utilizza key hendler ed è gestita da AWS
+ SSE-KMS : sfrutta il servizio gestito da AWS per lagestione delle chiavi
+ SSE-C : gestione personale delle chiavi con cifratura in cloud
+ Client Side Incripted : cifratura da parte utente

## SSE-S3
(Server Side Encription) cioè la criptazione viene gestita server side. Lo standard utilizzato è AES-256 e viene mostrato come : "x-amz-server-side-encriptyon" : "AES256" nell'header.
In pratica quando effettuiamo l'upload di un file su s3 inseriamo nell'header la stringa precedente. S3 leggerà questa stringa nell header e provvederà a criptare il file con AES 256 e la chiave gestita da S3. 

## SSE-KMS
Come per SSE-S3 questo è un metodo di encription lato server. Si basa sullo scambio di chiavi generato da un identificazione di un utente e un servizio di traking delle operazioni (audit trail). Una volta arrivato al server verrà nuovamente crittografato. nell'header della richiesta bisogna inserire come parametro : "x-amz-server-side-encriptyon" : "aws:kms".

## SSE-C 
Sempre una criptazione lato server solo che le chiavi non sono gestite da AWS ma da un servizio esterno. In questo caso AWS non salva la chiave utilizzata per la criptazione e quindi una buona norma utilizzare un protocollo di comunicazione sicuro come HTTPS poiche verrà passato oltre al file anche la chiave con la quale verranno criptati i dati. Come per l'upload anche per il download dovremo rifornire le chiavi per poter ottenere nuovamente l'oggetto richiesto.


## Client Side Encription
Con questa modalità sarà il client che si occuperà della criptazione dei file prima di caricarli in S3 e AWS non è responsabile della loro gestione per quanto riguarda la crittografia.

**In generale nelle impostazioni abbiamo solo le opzioni S3 e KMS poiche le ultime due vengono implementate lato client e AWS non ha alcuna responsabilità**

## Encription in transit
S3 espone endpoint di tipo HTTP e HTTPS basati sui ceritficati SSL e TLS. In generale è una buona norma utilizzare HTTPS. 

# Sicurezza 
La sicurezza su S3 si basa sul IAM quindi su Users, sulle risorse. Tramite l'IAM dobbiamo indicare quali utenti/servizi possono aver accesso. Per le risorse abbiamo le **Bucket policies** che definiscono regole di accesso nella dashboard di S3, con queste regole possiamo utilizzare anche sistemi **cross account** (es credenziali di google), supporta anche un controllo a grana più fine tramite le **OACL** (Object Access Control List) e un ultima tipologia le **BACL** (Bucket Access Control List).
Tutte queste policie non sono mutuamente esclusive anzi se un utente ha le credenziali di IAM se le Bucket policies non lo consentono esso non riuscirà ad accedervi.

## Bucket Policies
Sono policie basate sullo standard JSON. Tramite queste policie possiamo :
+ applicare policie ai bucket o a singoli oggetti
+ definire quali API possono accedere
+ definire gli effetti dell'approvazione o della negazione all'accesso

Generalmente vengono utilizzate per garantire l'accesso a bucket pubblici o per rafforzare la cifratura degli upload per evitare brute force o per concedere dei permessi ai cross account.

Tramite le bucket policies possiamo rendere un bucket inaccessibile dall'esterno, durante la creazione del bucket vengono presentate alcune restrizioni per gli accessi quali : 
+ Blocco di accessi concessi tramite nuove ACL
+ Blocco di accessi concessi tramite qualsiasi lista ACL
+ Blocca gli accessi pubblici a bucket e oggetti concessi tramite qualsiasi policy pubblica del bucket o del punto di accesso
+ Blocca gli accessi pubblici a bucket e oggetti concessi tramite nuove policy pubblica del bucket o del punto di accesso

Questi blocchi sono stati implementati per evitare la perdita di dati aziendali

## Altri metodi 

Possiamo implementare la sicurezza tramite : 
+ Networking : 
    + VPC endpoint (per gli accessi esterni alla VPC di altri servizi AWS)
+ Login e Audit 
    + verifica di identità (Access log che può essere storicizzato in un altro bucket) e della correttezza dei dati 
    + Le richieste tramite API possono essere registrate tramite CloudTrail
+ User 
    + richiedere la MFA (Multi Factor Authentication ) che può essere richiesta nella sezione versioning per l'eliminazione di obj
    + Pre-signed URL sono degli URL con al loro interno le chiavi di autenticazione valide per un tempo limitato (utilizzabili per utenti che devono avere accesso alla risorsa per un tempo prestabilito).

![](../immagini/PolicieS3.png)

Una volta creato il bucket possiamo cliccare su di esso e andare nella sezione autorizzazioni per poter modificare tutti i criteri di sicurezza definiti precedentemente. nella sezione Policie bucket possiamo inserire un codice JSON per definire in maniera specifica una policie di accesso come ad esempio la segunete :  
```json
{
  "Version":"2012-10-17",
  "Statement":[
    {
      "Sid":"AddCannedAcl",
      "Effect":"Allow",
    "Principal": {"AWS": ["arn:aws:iam::111122223333:root","arn:aws:iam::444455556666:root"]},
      "Action":["s3:PutObject","s3:PutObjectAcl"],
      "Resource":"arn:aws:s3:::awsexamplebucket1/*",
      "Condition":{"StringEquals":{"s3:x-amz-acl":["public-read"]}}
    }
  ]
}
```
Come potete vedere abbiamo innanzi tutto l'indicazione della versione e nello statement andiamo a deifinire la regola che dice : sid il nome della regola che ha un effetto di approvazione nel caso in cui i due utenti IAM relativi all'account "tanti numeri" ne richiedano l'accesso ed eseguono azzioni di upload di un oggetto o di una lista di controllo sulla risorsa definita dal nome del bucket *awsexamplebucket1* e a tutto il suo contenuto a condizione che la stringa inviata nell'header della richiesta alla voce di s3:x-amz-acl sia uguale a public-read.

Nel caso in cui non si voglia apprendere il linguaggio di definizione delle policie possiamo utilizzare [**AWS Policie Generator**](https://awspolicygen.s3.amazonaws.com/policygen.html).


# S3 Websites
I bucket possono essere utilizzati per la storicizzazione dei siti. Tali bucket sono raggiungibili tramite l'URL : `(nome-buket).s3-website.(AWS-region).amazonaws.com` o `(nome-buket).s3-website-(AWS-region).amazonaws.com` .  
Per poter rendere il buket un sito statico bisogna impostarlo, nelle proprietà, come static website hosting, inoltre abbiamo che il bucket di default viene impostato come privato quindi bisogna concedere l'accesso publico. Tutto ciò non è ancora sufficente a raggiungere il nostro sito dall'esterno poiché non abbiamo definito la bucket policy. 
Un esempio di policy è la seguente : 
```json
{
    "Version": "2012-10-17",
    "Id": "Policy1603382049413",
    "Statement": [
        {
            "Sid": "Stmt1603382045838",
            "Effect": "Allow",
            "Principal": "*",
            "Action": "s3:GetObject",
            "Resource": "arn:aws:s3:::prova-per-test/*"
        }
    ]
}
```

## CORS
Per comprendere al meglio il concetto del CORS (Cross Origin Resource Sharing) introduciamo il concetto di origin schema, L'origin non è altro che la terna formata da prototocollo, dominio e porta. Il CORS è un blocco che viene implementato lato browser e lato server per avere il controllo su collegamenti che non avvengono tra due enti specifici, ad esempio se un client comunica con un server all'indirizzo https://www.esempio.com e vine effettuata una richiesta che punta al sito https://www.altroesempio.com quest'ultima verrà bloccata. É possibile evadere questo meccanismo solo se il server abilità il CORS tramite un messaggio nell'header del pacchetto con la proprietà : Access-Controll-Allow-Origin 

![](../immagini/corsfasi.png)  
L'immagini esprime al meglio il concetto sulle fasi di comunicazione.

Per S3 possiamo avere il caso in cui un sito rimandi un file o in generale una richiesta ad un bucket e quindi bisogna far in modo che il bucket risponda nella maniera più consona al caso.

Per poter impostare il controllo CORS sull'origine che sta richiedendo l'accesso bisogna andare nella segunete sezione :
![](../immagini/DashCORSAWS.png)  

Qui inserire il settaggio segunete : 
```xml
<CORSConfiguration>
 <CORSRule>
   <AllowedOrigin>http://www.example1.com</AllowedOrigin>

   <AllowedMethod>PUT</AllowedMethod>
   <AllowedMethod>POST</AllowedMethod>
   <AllowedMethod>DELETE</AllowedMethod>

   <AllowedHeader>*</AllowedHeader>
 </CORSRule>
 <CORSRule>
   <AllowedOrigin>http://www.example2.com</AllowedOrigin>

   <AllowedMethod>PUT</AllowedMethod>
   <AllowedMethod>POST</AllowedMethod>
   <AllowedMethod>DELETE</AllowedMethod>

   <AllowedHeader>*</AllowedHeader>
 </CORSRule>
 <CORSRule>
   <AllowedOrigin>*</AllowedOrigin>
   <AllowedMethod>GET</AllowedMethod>
 </CORSRule>
</CORSConfiguration>
```
In questo esempio vediamo come è possibile gestire il CORS per vari siti di origine.

# Modello di consistenza di S3
I bucket creano delle copie quando effettuiamo una PUT. Per evitare problemi di consistenza di dati S3 implementa alcuni metodi per garantire la coerenza.
+ Effettuando un PUT dobbiamo attendere la risposta dell caricamento prima di richiedere una GET altrimenti otterremo una risposta 404.
+ nel caso di un DELETE o UPDATE potremmo ottenere un elemento della versione precedente se esso non è ancora stato caricato completamente

Queste situazioni rientrano nel concetto di **Consistency Model**


# MFA delete 
Senza l'attivazione del MFA non sono possibili le seguenti operaizoni : 
+ eliminare permanentemente un oggetto dal bucket
+ sospendere il versioning

Mentre senza la MFA possiamo comunque : 
+ abilitare il versioning
+ assegnare il marcatore di eliminazione

Solo il root account può abilitare o disabilitare l'MFA delete e per attivare questa impostazione possiamo farlo solo tramite il CLI, attualmente non ci sono altri metodi.

Per poter attivare questo tipo di controllo sui bucket dobbiamo :
+ attivare l'MFA per l'utente root
+ generare delle chiavi di accesso (opzione sconsigliata ma necessarie per questa opzione)
+ configuriamo il cli aws e configuriamo un profilo root
+ Per abilitare l'MFA Delete bisogna lanciare da cli il segunte comando :   
	`aws s3api put-bucket-versioning --bucket <nome_bucket> --versioning-configuration Status=Enabled,MFADelete=Enabled --mfa "arn::<nome_del_device_mfa>  <codice_del_dispositivo>" --profile <nome_profilo_root>`

In questo modo solo l'utente root può eliminare i file definitivamente o sospendere il versioning. Per disabilitare questa opzione basta rilanciare lo stesso comando solo con MFADelete=Disabled e aggiornare il codice dispositivo.

# S3 Access log
Attraverso i log di accesso richiediamo ad s3 di salvare tutte le richieste fatte al bucket in un altro bucket cosi da poter effettuare delle analisi sugli accessi. Questi log possono essere analizzati da tool di analisi accessi o dal servizio di AWS Athena. Il formato nella quale AWS salva i log può essere trovato all'indirizzo : https://docs.aws.amazon.com/AmazonS3/latest/dev/LogFormat.html . 

Tra le regole principali per la gestione di questo sistema troviamo : mai selezionare come bucket di log lo stesso che vogliamo controllare altrimenti le dimensioni crescerebbero enormemente.


Per abilitare un bucket come access log basta semplicemente andare nelle proprietà del bucket che vogliamo monitorare e impostare la proprietà server access logging su enable scegliendo il bucket che ci farà da log e abbiamo la possibilità di inserire un prefisso per ottenere info specifiche di uno specifico bucket. Una volta impostato ogni azione fatta viene registrata in un file di testo che conterrà informazioni utili al fine della comprensione delle operazioni fatte. 

# S3 Replication 
Abbiamo due tipologie di replicazione per il bucket e sono : CRR (Cross Region Replication) e SRR (Same Region Replication).
Per poter abilitare la replicazione dei bucket bisogna : 
+ abilitare il versioning in entrambi i bucket
+ Impostare se volgiamo un SRR o un CRR
+ I bucket possono avere differneti account
+ La copia avviene in maniera asincrona
+ Bisogna creare un ruolo nell'IAM affinche il bucket di origine possa copiare gli oggetti nel secondo bucket 

I casi d'uso più comuni sono : per il CRR, una copia per ridurre la latenza,  per l'SRR, un aggregatore di log

Dopo aver avviato la replicazione sono i nuovi oggetti verranno replicati questa opzione non è retroattiva, l'eliminazione non sarà definitiva nei bucket di replica ne verrà applicato un marker. Non è possibile avviare una catena di replicazioni. 

Per impostare una replicazione da un bucket origin ad uno copy bisogna : 
+ dal bucket origin andiamo su management
+ Replication 
+ add rule (dove possiamo indicare ad esempio i prefissi dei file da replicare o di copiare l'intero bucket)
+ Selezioniamo il bucket di destinazione 
+ Selezionare un ruolo o crearne uno 

# Pre-signed URL
Viene utilizzato per accedere ai file contenuti in un bucket quando esso è non publico e permette di avere un URL dove al suo interno è contenuta una chiave. Principalmente viene utilizzato per i download (tramite CLI ) e può essere utilizzato anche per gli upload (solo tramite SDK). Di defautl questo URL ha una validità di 3600s. Quando si crea un URL prefirmato l'utilizzatore avra gli stessi privilegi dell'utente che lo ha generato quindi in generale può effettuare operazioni di GET e di PUT. In genere si utilizza per dare accesso temporaneo ad un client che non ha diritti sul bucket e che normalmente non potrebbe accedere.

Per poter generare un URL prefirmato da cli bisogna :  
+ configurare il cli per creare URL prefirmati tramite il comando :   
	`aws configure set default.s3.signature_version s3v4`
+ Dopo aver abilitato la firma bisogna sapere l'url del file che si vuole concedere e lanciare il seguente comando :   
	`aws s3 presign s3://<nome_bucket>/<nome_file> --expires-in 300 --region <regione_dove_risiede_il_bucket>`

# Classi di storage in S3
Ci sono varie soluzioni per storicizzare dati in S3 e sono :
+ Standard : 
	+ per usi generici
	+ alta avviabilità e durabilità
	+ fino a 10M obj
	+ resiste fino a due guasti in contemporanea
+ Inferquent Access (IA) : 
	+ Per accessi sporadici ma rapidi
	+ alta avviabilità e durabilità
	+ resiste fino a due guasti in contemporanea
	+ casi d'uso : datastore per disaster recovery, backup 
+ One-Zone-Infrequent Access : 
	+ come per IA sono storicizzati in un unica AZ
	+ supporta SSL
	+ casi d'uso : datastore secondario per disaster recovery, backup, in generale per tutti quei dati recuperabili  
+ Intelligent Tiering : 
	+ gestisce lo spostamento tra classi di storage nello specifico tra standard e IA
	+ seleziona automaticamente quali file spostare
	+ si paga una quota mensile per il monitoraggio 
+ Glacier
	+ per storage lunghi (10y)
	+ non salva in oggetti ma in archivi
	+ ha 3 opzioni di recupero
		+ Expirater (da 1 a 5 min)
		+ Standard (da 3 a 5 h)
		+ Bulk (da 5 a 12 h)
	+ Tempo minimo di storage sono 90gg
+ Glacier Deep Archive
	+ come glacier normale ma i tempi di recupero sono :
		+ Standard (12h)
		+ Bulk (48h)
	+ tempo minimo di storage sono 180gg

Possiamo selezioanre la classe di storage quando carichiamo un file o nelle proprietà del file.

## Life cycle Poicies
Vengono utilizzate quando vogliamo cereare dei metodi che facciano cambiare la classe di storage agli oggetti. 
Le opzioni supportabili sono : 
+ Transition action : 
	+ muovi gli oggetti nel IA dopo 60gg dopo la creazione
	+ sposta gli oggetti in glacier dopo 6 mesi 

+ Expiration action : 
	+ Elimina i log file dopo 365gg
	+ Elimina le vecchie versioni dei file

Si possono impostare le azioni a livello di prefisso o per tag. Per impostare una regola basta : 
+ andare nella sezione management del bucket
+ lifecycle
+ add rule
+ inserire : nome, filtro per tag o prefisso, scelta tra versione corrente o versioni precedenti, aggiungi transizione e giorni, aggiungi Expiration per versioni precedenti o attuali e per multipart file incompleti.


# Prestazioni 
Partiamo dall'autoscaling che ha una latenza di 200ms, quindi molto rapida. Per prefisso possiamo effettuare 3500 operazioni di PUT,COPY,POST,DELETE al secondo e 5500 operazioni di GET, HEAD al secondo. Non ci sono limiti di prefissi per bucket.

## KMS limitazioni
Quando utilizziamo la criptazione KMS questa effetturà delle richieste aggiuntive che abbaseranno il numero di richeste al secondo per obj. Quando si effettua l'upload verrà chiamata l'API **GenerateDataKey** mentre quando effettuiamo il download verrà riciamata l'API **Decrit** queste chiamate vengono conteggiate nei limiti. In generale il limite di richieste ad un bucket dipende dalla regione nella quale giace il bucket e questo limite non è incrementabile.

Per incrementare le prestazioni esistono vari metodi tra cui : 
+ Multi part upload 
	+ viene raccomandato per file di dimensioni superiori ai 100MB 
	+ É obbligatorio per file superiorio ai 5GB
	+ L'operazione consiste nel dividere il file in parti ed effettuare un upload in parallelo, poi sarà S3 a riunificare il file.
+ S3 Transfert acceleration (sopo per upload)
	+ si basa sull'utilizzo delle edge location e sulla rete privata AWS, in pratica invece di caricare un file direttamente su S3 lo inviamo ad un Edge che poi lo indirizzerà sulla rete privata di AWS al bucket richiesto. Questa opzione viene utilizzata per trasferimenti di file quando il client ed il bucket sono molto lontani (diveri continenti)
+ S3 Byte-Range-Fetchs 
	+ Viene utilizzato per il download e consiste nel effettuare più GET in parallelo per ottenere parti di un file più grande. Queste parti vengono definite tramite un range e sono stati implementati dei sistemi per il controllo del file una volta arrivato a destinazione. Questa opzione viene utilizzata anche per repuerare parti di file come metadata ecc.

## Select 
Come per i DB relazionali anche S3 è in grado di inviare solo una porzione di dati invece che tutto il file cosi da ridurre tempi di download. 

# Notifiche Eventi
Alcuni eventi che si vengono a presentare nel bucket possono avviare dei trigger di operazoni. Quando viene creato, eliminato, replicato ecc un obj nel bucket si possono creare delle regole di notifica che avvieranno i servizi : SNS (Simple Notification Service), SQS (Simple Queue Service) e Lambda function. Queste notifiche in genere vengono generate in una manciata di secondi. Per poter eseguire le notifiche bisogna attivare il versioning nel bucket. 

Per impostare una regola bisgona : 
+ Andare nelle proprietà del bucket
+ Selezionare Event
+ Add notification 
+ inserire : nome, evento del bucket, prefisso, suffisso (estensione), impostare il reciver del messaggio.
+ concedere i permessi ad S3 per avviare la comunicazione.

# AWS Athena 
É un servizio serverless per l'ananlisi degli obj di S3. Si basa sulle classiche operazioni SQL senza che gli obj siano tabelle di un DB. Supporta i driver JDBC e ODBC. I costi sono definiti dal numero di query. Supporta file di tipo : csv, json, orc, Avro, Parquet. Come motore SQL esegue Presto. Gli utilizzi di Athena sono : Buisiness Intelligence, analytics, reporting, analyze and query sui VPC flow log, ELB log, CloudTrail, Cloudfront

Per poter utilizare il servizio basta ricercarlo nella dashboard. 
una volta aperto possiamo iniziare il settaggio : 
+ Andiamo sul query editor
+ andiamo su setting 
+ inseriamo l'ARN del bucket nella quale vogliamo inserre i risultati delle interrogazioni
+ Selezioniamo se vogliamo la criptazione delle query e se vogliamo l'autocompletamento

Una volta impostato il servizio possiamo ad esempio gestire gli accessi ad un bucket S3 utilizzando le seguenti query :  
```sql
CREATE database s3_access_log_db;
CREATE EXTERNAL TABLE IF NOT EXISTS s3_access_logs_db.mybucket_logs(
         BucketOwner STRING,
         Bucket STRING,
         RequestDateTime STRING,
         RemoteIP STRING,
         Requester STRING,
         RequestID STRING,
         Operation STRING,
         Key STRING,
         RequestURI_operation STRING,
         RequestURI_key STRING,
         RequestURI_httpProtoversion STRING,
         HTTPstatus STRING,
         ErrorCode STRING,
         BytesSent BIGINT,
         ObjectSize BIGINT,
         TotalTime STRING,
         TurnAroundTime STRING,
         Referrer STRING,
         UserAgent STRING,
         VersionId STRING,
         HostId STRING,
         SigV STRING,
         CipherSuite STRING,
         AuthType STRING,
         EndPoint STRING,
         TLSVersion STRING
) 
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.RegexSerDe'
WITH SERDEPROPERTIES (
         'serialization.format' = '1', 'input.regex' = '([^ ]*) ([^ ]*) 
		 \\[(.*?)\\] ([^ ]*) ([^ ]*) ([^ ]*) ([^ ]*) ([^ ]*) \\\"([^ ]*) 
		 ([^ ]*) (- |[^ ]*)\\\" (-|[0-9]*) ([^ ]*) ([^ ]*) ([^ ]*) ([^ ]*) 
		 ([^ ]*) ([^ ]*) (\"[^\"]*\") ([^ ]*)(?: ([^ ]*) ([^ ]*) ([^ ]*)
		  ([^ ]*) ([^ ]*) ([^ ]*))?.*$' )
LOCATION 's3://doc-example-bucket/prefix/'
```
Alla voce location sostituiamo il bucket dove salviamo i log di accesso. Una volta creata questo DB possiamo andare ad effettuare delle interrogazioni per analizzare specifiche condizioni come ad esempio le ultime richeiste HTTP effettuate nei vari bucket alla quale fanno riferimento i log. Queste operazioni non hanno bisogno di trasformare il contenuto in un DB e quindi verranno salvati solo i risultati delle interrogazioni. 
Un esempio pratico di interrogazione è il seguente : 
```sql
SELECT requesturi_operation, httpstatus, count(*) FROM "s3_access_log_db"."mybucket_logs" GROUP BY requesturi_operation, httpstatus;
```
Con questa query possiamo investigare quanti httpstatus sono avvenuti e il loro numero separato per stato.

Un ulteriore esempio è il segunete : 
```sql
SELECT * FROM  "s3_access_log_db"."mybucket_logs" WHERE httpstatus='403';
```
Con questo codice possiamo andare ad analizzare su quale bucket, la data, ecc dove è avvenuta una risposta 403 nei log.
In generale se ricerchiamo sul web "Athena analyze servizio log" troveremo tutta una serie di codici che ci permettono di generare il DB per specifico servizio e anche le interrogazioni tipiche per esso.


# Object Lock and Glacier Vault Lock
Sono due metodi per creare oggetti che devono solo essere letti e mai modificati. 
+ Object Lock
	+ Usa il metodo WORM (Write Once Read Many)
	+ Blocca l'obj per uno specifico lasso temporale
+ Glacier Vault Lock 
	+ Usa il metodo WORM (Write Once Read Many)
	+ si basa su di una policy di modifica (non è possibile modificare la policy una volta impostata)
	+ Utile per rispettare vincoli e standard